---
layout: post
title: "ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense"
subtitle: ""
bg: white
color: black
style: justify
---


|[![arXiv preprint](./img/posts/athena_preprint.png "arXiv preprint"){:height="130px" width="110px" border="1px solid blue"}](https://arxiv.org/abs/2001.00308 "arXiv preprint")||[![code](./img/posts/athena_code.png "code on GitHub"){:height="130px" width="110px" border="1px solid blue"}](https://github.com/softsys4ai/athena "code on GitHub")||[![class project](./img/posts/class_project.png "class project"){:height="130px" width="110px" border="1px solid blue"}](https://github.com/csce585-mlsystems/project-athena "class project")||[![tutorial](./img/posts/tutorial_craftAE_zk.png "Generate zero knowledge AEs tutorial"){:height="130px" width="110px" border="1px solid blue"}](https://github.com/csce585-mlsystems/project-athena/blob/master/notebooks/Task1_GenerateAEs_ZeroKnowledgeModel.ipynb "Generate zero knowledge AEs tutorial")|
|:------------:|:------------:|:------------:|:------------:|:------------:|:------------:|:------------:|
|[arXiv <br> preprint](https://arxiv.org/abs/2001.00308)|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[code <br> on GitHub](https://github.com/softsys4ai/athena)|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[ATHENA for <br> class project](https://github.com/csce585-mlsystems/project-athena)|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|[tutorial: <br> generate AEs <br> in ZK model](https://github.com/csce585-mlsystems/project-athena/blob/master/notebooks/Task1_GenerateAEs_ZeroKnowledgeModel.ipynb)|


--------
## Defense as a framework
Though deep learning systems have achieved impressive success in a wide range of domains like computer vision and natural language processing, they are highly vulnerable to adversarial examples. An adversarial example is an input artifact that is crafted from legitimate data by adding small and human-imperceptible perturbations, aiming to covertly force the deep learning system to produce an incorrect output. The vulnerability to adversarial examples can lead to a series of consequences, especially in security-critical tasks. For example, an object detector on a self-driving vehicle may incorrectly recognize an adversarial perturbed stop sign as a speed limit. 

In traditional machine learning, the test set is assumed to be drawn from an identical distribution of the training set. However, in adversarial machine learning, the defenders face an open-ended problem, in which an input can be an independent-identical-distribution (i.i.d.) sample (i.e., a legitimate sample) or an out-of-distribution (o.o.d.) sample (i.e., an adversarial example). Thus, research on adversarial defense is difficult. The threat of the adversarial examples has inspired a sizable body of research on various defense techniques. With the assumption on the specific known attack(s), most of the existing defenses, although effective against particular attacks, can be circumvented under slightly different conditions, either a stronger adaptive adversary or in some cases even weak (but different) adversaries. The "arms race" between the attacks and defenses leads us to this central question:

**`How can we, instead, design a defense, not as a technique, but as a framework that one can construct a specific defense considering the niche tradeoff space of robustness one may want to achieve as well as the cost one is willing to pay to achieve that level of robustness?`**

||||
|:-------------------:|:-------------------:|-------------------|
|![framework](./img/posts/athena_fwk_test.png){:width="460px" height="auto"}||To address the question, we propose **ATHENA** --- a framework for building an ensemble defense on top of a large population of *diverse* transformations. In this framework, each WD is an image classifier that was independently trained on a transformed dataset at the training phase. At the test phase, a WD first applies a transformation *`t`* on the original input **`x`** and then produces an output for the transformed input *`t`*`(`**`x`**`)`. Given an input **`x`**, an ensemble (i) first collects outputs from all WDs, and then (ii) uses an *ensemble strategy* such as majority voting and averaging outputs from WDs to compute the final output.|
|:-------------------:|:-------------------:|:-------------------:|
|Figure 1. ATHENA at deployment phase|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;||

--------
## Ensemble based on diverse transformations

|![transformations as weak defenses](./img/posts/trans_as_wd.png){:width="440px" height="auto"}||![ensemble of diverse transformation are effective](./img/posts/athena_motivation.png){:width="620px" height="auto"}|
|:--------------:|:---------------------:|:------------------:|
|Figure 2. Transformation as weak defense|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|Figure 3. Many diverse transformations can result in an effective ensemble|

In computer vision, a transformation is an image processing function. By distorbing its input, a transformation changes the adversarial optimized perturbations and thus making the perturbations less effective. However, the effectiveness of a single type of transformation varies on attacks and datasets. By mitigating the perturbations in different ways such as adjusting angles or position of the input, adding or removing noises, a collection of diverse transformations provides robustness against various attacks. Thus, the "Diverse" ensemble achieves the lowest error rate in most cases, especially for tasks on CIFAR-100. 

Ensembling diverse transformations can result in a robust defense against a variety of attacks and provide a *tradeoff space*, where one build a *higher robustness* ensemble by adding more transformation or build an ensemble with *lower overhead and cost* by utilizing fewer transformations.

--------
## Results

### Zero knowledge threat model

|![error rates zero knowledge model](./img/posts/error_zk.png){:width="1100px", height="auto"}|
|:---------:|
|Figure 4. Evaulations of ATHENA and baseline defenses against various attack variants in the context of zero-knowledge threat model|

The effectiveness of individual WDs (each associated to a transformation) varies across attack methods and magnitudes of an attack. While a large population of transformations from a variety of categories successfully disentangle adversarial perturbations generated by various attacks. As presented in Figure 4., the variation of individual WDs' error rates spans wider as the perturbation magnitude become stronger for a selected attack. By utilizing many diverse transformations, with ATHENA, we build effective ensembles that outperform the two state-of-the-art defenses --- PGD adversarial training (PGD-ADT) and randomly smoothing (RS), in all cases.

### Black-box threat model

#### Transfer-based approach

|![Evaluations on transfer-based black-box attack](./img/posts/eval_bb_trans.png){:width="680px" height="auto"}|
|------|
|Figure 5. Evaluations of ATHENA, in terms of the attack's transferability with various budgets.|

Although the transferability rate increases as the budget increases, the increasing of the drop in the transferability rate from the undefended model (UM) to ATHENA indicates that ATHENA is less sensitive to the perturbation. Ensembling from many diverse transformations provides tangible benefits in blocking the adversarial transferability between weak defenses, and thus enhances model's robustness against the transfer-based black-box attack. 


#### Gradient-direction-estimation-based approach

|![Evaluations on gradient-direction estimation black-box approach](./img/posts/eval_bb_hsja.png){:width="640px" height="auto"}||![hsja samples](./img/posts/samples_cifar100_bb_linf.png){:width="450px" height="auto"}|
|:------|:------|:------|
|Figure 6. Evaluations of ATHENA, in terms of mean distance and distance drop ratio on various query budgets.|&nbsp;&nbsp;&nbsp;&nbsp;|Figure 7. Adversarial examples generated by HSJA ($l_{\infty}$ norm) with a budget of 5000 queries against the undefended model (UM) and ATHENA.|

Hop-Skip-Jump attack (HSJA) generates adversarial examples by querying the output labels from the target model for the perturbed images. Compared to that generated based on the UM, the adversarial examples generated based on ATHENA are much further to the corresponding benign samples. As the query budget increases, the distances of the UM-targeted AEs drop much more significantly that that of the ATHENA-targeted AEs. Therefore, ATHENA provides robustness against HSJA with varying query budgets.

### Gray-box and white-box threat model

#### Greedy approach



|![Evaluations on greedy white-box attack](./img/posts/eval_wb_greedy.png){:width="500px" height="auto"}||![greedy white-box samples](./img/posts/samples_cifar100_wb.png){:width="450px" height="auto"}|
|:------|:------|:------|
|Figure 7. Evaluations of ATHENA against the greedy white-box approach.|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|Figure 8. Adversarial examples generated by greedily aggregating adversarial perturbations from individual weak defenses (WDs).|
|:------|:------|:------|
|![Cost of greedy white-box attack](./img/posts/eval_wb_cost.png){:width="460px" height="auto"}||![Detect greedy white-box adversarial examples](./img/posts/eval_wb_detector.png){:width="500px" height="auto"}|
|:------|:------|:------|
|Figure 9. Computational cost (per sample) of greedy white-box approach.|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|Figure 10. Adversarial examples generated by the greedy white-box approach are easily detected.|


As expected, stronger AEs are generated by the greedy white-box attack with a looser constraint on the dissimilarity threshold (Figure 7). However, such success comes at a price: with the largest threshold we examined (the maximal dissimilarity is $1.0$), the greedy attack has to spend $310x$ in time to generate adversarial example for a single input (Figure 9). This provides a tradeoff space, where realizations of ATHENA that employ larger ensembles incur more expensive defense to attack. Moreover, the generated AEs are distored heavily and very likely to be detected either by a human (Figure 8) or an adversarial detector (Figure 10).


#### Optimization-based approach

|As the adversary become more knowledgable to ATHENA (i.e., having access to more WDs), it can launch more successful attack without even increasing the perturbations. However, the computational cost of AE generation increases as well. The attacker has the choice to sample more random transformations and a choice to a distribution of a large population and diverse transformations, in order to generate stronger AEs. However, this will incur a larger computational cost as well.|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|![evaluations of ATHENA against the optimization-based white-box attack](./img/posts/eval_wb_optimization.png){:width="460px" height="auto"}|


--------
## Acknowledgement
* Google via GCP cloud research credits
* NASA (EPSCoR 521340-SC001)
* Research Computing Center at University of South Carolina


--------
## How to cite

### Citation

Ying Meng, Jianhai Su, Jason M O'Kane, and Pooyan Jamshidi. *ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense*. arXiv preprint arXiv: 2001.00308, 2020.


### Bibtex
```
@article{meng2020athena,
      title={ATHENA: A Framework based on Diverse Weak Defenses for Building Adversarial Defense},
      author={Ying Meng and Jianhai Su and Jason M O'Kane and Pooyan Jamshidi},
      journal={arXiv preprint arXiv:2001.00308},
      year={2020}
}
```


--------
## Talks
<iframe src="//www.slideshare.net/slideshow/embed_code/key/Cb31RI6bvqz2YE" width="480" height="300" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>

<div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/pooyanjamshidi/ensembles-of-many-diverse-weak-defenses-can-be-strong-defending-deep-neural-networks-against-adversarial-attacks" title="Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks Against Adversarial Attacks" target="_blank">Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks Against Adversarial Attacks</a> </strong> from <strong><a href="https://www.slideshare.net/pooyanjamshidi" target="_blank">Pooyan Jamshidi</a></strong> </div>

<script async class="speakerdeck-embed" data-id="5e56d27c80a94fb59cd019305eabf53f" data-ratio="1.8" src="//speakerdeck.com/assets/embed.js"></script>

--------
## Data availability

The [code posted on github](https://github.com/softsys4ai/athena) contains the analysis needed to reproduce the results in the paper. It also includes scripts for setting up all the dependencies using conda, and scripts to download the datasets and models used in the analysis of the paper. The models and datasets can also be downloaded directly (in keras, pickle, and pytorch formats) here:
* [CNN models for MNIST dataset](https://zenodo.org/record/4121901#.X5MPOnVKhhE)
* [SVM mdoels for MNIST dataset] *To be upload*
* Wide ResNet models and ResNet with Shake-Shake regularization models for CIFAR-100 dataset  *To be upload*
* AEs in the zero-knowledge model *To be upload*
* AEs in the black-box model *To be upload*
* AEs in the white-box model *To be upload*