---
layout: default
---

<!--
<div class="home">

<h1>{{ site.subtitle }}</h1>
{% include image.html url="" caption="sample adversarial examples" align="center" %}
{{ site.description }}
{% include image.html url="./_images/athena_fwk.png" caption="" width=900 align="center" %}
{{ site.description }}
</div>
-->

<!DOCTYPE html>
<html dir="ltr" lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="ATHENA: a framework for building adversarial defense">
    <title>{{ site.title }}</title>
    <link rel="stylesheet" href="/athena/_css/main.css">
    <link rel="canonical" href="http://localhost:4000/athena">
    <link rel="alternate" type="application/rss+xml" title="Athena" href="http://localhost:4000/athena/feed.xml" />
    <link rel='stylesheet' id='open-sans-css'  href='//fonts.googleapis.com/css?family=Open+Sans%3A300italic%2C400italic%2C600italic%2C300%2C400%2C600&#038;subset=latin%2Clatin-ext&#038;ver=4.2.4' type='text/css' media='all' />
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:600italic,600,400,400italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    <meta property="og:image" content="./_images/athena_fwk.png"/>
</head>
<body>
    <h1>{{ site.title }}: {{ site.subtitle }}</h1>
    <h3>{{ site.authors }}</h3>
    <div id="main">
	<p>
	To date, machine learning systems continue to be highly susceptible to <em>adversarial examples</em>, notwithstanding their attainment of state-of-the-art performance across a wide variety of domains, including speech recognition, object detection, and image classification. Adversarial examples, well-designed to fool machine learning systems, are typically generated by adding small, human-imperceptible perturbations to benign samples.
	</p>
	<p>
	There has been extensive research on developing defense techniques against adversarial attacks; however, they have been mainly designed for specific model families, therefore, they cannot be easily extended to new domains. In addition, the current defense techniques in almost all cases assume specific known attack(s) and evaluated in weak threat models, in which the attacker has no or limited access to the defense model. As a result, these defenses can be circumvented under slightly different conditions, either a stronger adaptive adversary or a different adversary. The "arms race" between the attacks and defenses leads us to pose this central question:
	</p>
	<p>
	<strong>How can we, instead, design a defense, not as a techique, but as a framework that one can construct a specific defense considering the niche tradeoff space of robustness one may want to achieve as well as the cost one is will to pay achieve that level of robustness?</strong>
	</p>
	<p>
	To address this questin, we propose <em>ATHENA</em> --- an <em>extensible framework</em> for builind <em>generic</em> (and thus, broadly applicable) yet effective defenses against adversarial attacks.
	</p>
	<h2>ATHENA framework</h2>
	<figure>
	    <img src="./_images/athena_fwk.png" caption="The architecture of ATHENA." width="80%" align="center">
	    <figcaption>Figure 1. The architecture of ATHENA</figcaption>
	</figure>
	<!-- {% include image.html url="./_images/athena_fwk.png" caption="Figure 1. The architecture of ATHENA." width="40%" align="center" %} -->
	<p>The design philosophy behind ATHENA is based on ensemble of many <em>diverse weak defenses</em> (WDs). Each WD, the basic building block of the framework, is a machine learning classifier (e.g., DNN, SVM) trained on a transformed dataset <em>D={(t(<strong>x</strong>), y)}</em> independently at the training phase (Figure 1. (1)). Given an input <strong>x</strong>, a trained WD first applies a transformation <em>t</em> on <strong>x</strong> and then produces an output for the transformed input <em>t(x)</em>. For a given input <strong>x</strong>, an ensemble first collects predicted outputs from all WDs then determines the final output, using some <em>ensemble strategy</em> such as majority voting or averaging the predictions from the WDs (Figure 1. (2)).
        </p>

	<h2></h2>

        <h2>Publication</h2>
        <ol>
            <li>
               {{ site.authors }}. <a href="https://arxiv.org/abs/2001.00308">Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks against Adversarial Attacks</a>, arxiv 2001.00308, 2020.
            </li>
        </ol>

        <h2>Code</h2>
        <a href="https://github.com/softsys4ai/athena">https://github.com/softsys4ai/athena</a>
        <p></p>

        <h2>Talks</h2>
        <ol>
            <li>Ying Meng and Jianhai Su. <a href="https://cse.sc.edu/event/ensembles-many-weak-defenses-are-strong-defending-deep-neural-networks-against-adversarial">Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks against Adversarial Attacks</a>. University of South Carolina, Columbia, South Carolina, December 2019.
	    </li>
	    <li> Ensembles of Many Weak Defenses can be Strong: Defending Deep Neural Networks Against Adversarial Attacks. <strong><a href="https://www.slideshare.net/pooyanjamshidi" target="_blank">Pooyan Jamshidi</a></strong>
                <iframe src="//www.slideshare.net/slideshow/embed_code/key/Cb31RI6bvqz2YE" width="500" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> 
		</iframe> 
		<div style="margin-bottom:5px"><strong> <a href="//www.slideshare.net/pooyanjamshidi/ensembles-of-many-diverse-weak-defenses-can-be-strong-defending-deep-neural-networks-against-adversarial-attacks" title="Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks Against Adversarial Attacks" target="_blank">Ensembles of Many Diverse Weak Defenses can be Strong: Defending Deep Neural Networks Against Adversarial Attacks</a> </strong> from <strong><a href="https://www.slideshare.net/pooyanjamshidi" target="_blank">Pooyan Jamshidi</a></strong> 
		</div>

                <script async class="speakerdeck-embed" data-id="5e56d27c80a94fb59cd019305eabf53f" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
            </li>
        </ol>

        <h2>Future Works</h2>
        <p>Some future works based on Athena:</p>
        <ul>
            <li>Adversarial detector</li>
            <li>Automatically optimize weak defenses</li>
            <li>Representation learning in adversarial ML</li>
            <li>Defending against both sensitive and invariance adversarial perturbation</li>
        </ul>

        <h2>Acknowledgement</h2>
        <p>This project has been partially supported by:</p>
        <ul>
            <li>Google via GCP cloud research credits</li>
            <li>NASA (EPSCoR 521340-SC001)</li>
            <li> Research Computing Center at University of South Carolina</li>
        </ul>
    </div>
</body>
</html>
